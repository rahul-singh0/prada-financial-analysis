# Prada Financial Analysis 2013-2023
<h3>Objective</h3>
Prada, a renowed luxury fashion house, is known for its high-end clothing, leather goods, and accessories. This project aimed to analyse Prada's financial performance over the past decade, focusing on key metrics scuh as revenue, profit, and regional performance. By using tools such as PostgreSQL, Python, and Tableau, the project uncovers insights that reveal which regions, brands, and products drove growth, and provides recommendations for future business strategies.

<h3>Deliverables</h3>

- Comprehensive analysis of financial data from 2013-2023.
- Interactive visualisations in Tableau showcasing key financial metrics, including revenue, profit, brand performance, and regional breakdowns.
- Key insights and data-driven recommendations to guide future strategies for growth.

<h3>Tools and Technologies Used</h3>

- <b>PostgreSQL</b> for data storage and querying.
- <b>Python</b> for data processing, cleaning, and analysis.
- <b>Tableau</b> for data visualisation and dashboard creation.
- <b>Excel</b> for inital data exploration and validation.

<h2>Project Workflow</h2>
<h3>1. Define Scope and Metrics</h3>

The scope of the project focused on analysis financial perofrmance data, with the primary metrics including
- Total revenue.
- Net income.
- EBITDA (Earnings Before Interest, Taxes, Depreciation, and Amortization).
- Regional and brand revenue breakdown.
- Year-on-year growth in key metrics.

<h3>2. Data collection and Database Setup</h3>
To begin the project, I sourced Prada’s annual financial reports from 2013 to 2023. These reports provided the key data on financial performance, including revenue, profit, EBITDA, and regional contributions.

<h4>Data Extraction:</h4>
To streamline the extraction of key financial metrics from these reports, I utilised ChatGPT to gather and summarise critical figures such as total revenue, net income, EBITDA, and other relevant KPIs from each year’s report. This allowed for a more efficient extraction process across the 10-year period.

<h4>Data Organisation:</h4>

Once the data was collected, I organised it into separate tables using Excel, ensuring that each table represented a distinct aspect of Prada’s financial performance:
- <b>Financial Metrics:</b> Containing total revenue, EBITDA, net income, gross margin, EBIT, and total number of stores.
- <b>Brand Revenue:</b> Tracking revenue generated by Prada’s key brands: Prada, Miu Miu, and Church’s.
- <b>Product Revenue:</b> Categorising revenue based on product categories such as clothing, footwear, and leather goods.
- <b>Regional Revenue:</b> Displaying revenue by geographic regions including Americas, Asia Pacific, Europe, Japan, and the Middle East.
- <b>Sales Contribution:</b> Dividing sales into retail and wholesale percentages.

Once the tables were structured, I saved them in CSV format for further analysis using Python, SQL, and Tableau. The following entity relationship diagram (ERD) was created to represent the relationships between the tables and facilitate efficient querying.

<h4>Entity Relationship Diagram (ERD)</h4>

![entity_relationship_diagram](https://github.com/user-attachments/assets/3b0f0be6-2501-4db8-b9ea-737a5d4ce133)

This diagram showcases how the data tables are linked, with financial metrics serving as the primary data source, and supporting tables for brands, products, and regions.

<h4>Database and Table Creation</h4>

```sql 
CREATE DATABASE prada_financial_data;
```

```sql 
CREATE TABLE financial_metrics (
    year INT PRIMARY KEY,
    total_revenue_million_euro DECIMAL(10,2),
    ebitda_million_euro DECIMAL(10,2),
    net_income_million_euro DECIMAL(10,2),
    gross_margin_percent DECIMAL(5,2),
    ebit_million_euro DECIMAL(10,2),
    ebitda_margin_percent DECIMAL(5,2),
    total_stores INT
);

CREATE TABLE regional_revenue (
    year INT,
    region VARCHAR(50),
    revenue_million_euro DECIMAL(10,2),
    PRIMARY KEY (year, region),
    FOREIGN KEY (year) REFERENCES financial_metrics(year)
);

CREATE TABLE product_revenue (
    year INT,
    product_category VARCHAR(50),
    revenue_million_euro DECIMAL(10,2),
    PRIMARY KEY (year, product_category),
    FOREIGN KEY (year) REFERENCES financial_metrics(year)
);

CREATE TABLE brand_revenue (
    year INT,
    brand VARCHAR(50),
    revenue_million_euro DECIMAL(10,2),
    PRIMARY KEY (year, brand),
    FOREIGN KEY (year) REFERENCES financial_metrics(year)
);

CREATE TABLE sales_contribution (
    year INT PRIMARY KEY,
    retail_sales_percent DECIMAL(5,2),
    wholesale_sales_percent DECIMAL(5,2),
    FOREIGN KEY (year) REFERENCES financial_metrics(year)
);
```

```sql 
COPY financial_metrics FROM '/Users/rahul/Documents/Analyst/Projects/Prada Financial Analysis/Annual Reports/Tables/csv/financial_metrics.csv' DELIMITER ',' CSV HEADER;

COPY regional_revenue FROM '/Users/rahul/Documents/Analyst/Projects/Prada Financial Analysis/Annual Reports/Tables/csv/regional_revenue.csv' DELIMITER ',' CSV HEADER;

COPY product_revenue FROM '/Users/rahul/Documents/Analyst/Projects/Prada Financial Analysis/Annual Reports/Tables/csv/product_revenue.csv' DELIMITER ',' CSV HEADER;

COPY brand_revenue FROM '/Users/rahul/Documents/Analyst/Projects/Prada Financial Analysis/Annual Reports/Tables/csv/brand_revenue.csv' DELIMITER ',' CSV HEADER;

COPY sales_contribution FROM '/Users/rahul/Documents/Analyst/Projects/Prada Financial Analysis/Annual Reports/Tables/csv/sales_contribution.csv' DELIMITER ',' CSV HEADER;
```

<h3>3. Data Cleaning and Preparation with SQL and Python</h3>

Once the database was set up with the relevant tables, the next step was to clean and prepare the data for analysis. This was done by connecting to the PostgreSQL database, extracting the data, checking for missing values, and ensuring consistency in the dataset.

1.	Loading the Data into PostgreSQL:

- CSV files were loaded into the respective tables within the PostgreSQL database using the COPY command to populate each table with the relevant data from the annual reports.

2.	Python Connection to PostgreSQL:

- Using psycopg2, a connection was established to the PostgreSQL database from Python. This allowed data to be queried directly into pandas DataFrames for further cleaning and analysis.
```python 
import psycopg2
conn = psycopg2.connect(
    host="localhost",
    database="prada_financial_data",
    user="rahul"
)
```

3. Querying Data from Multiple Tables:

- SQL queries were written to fetch data from multiple tables. Each query was executed, and the results were loaded into individual pandas DataFrames for further processing.
- Example query:
```python 
query_financial = "SELECT * FROM financial_metrics"
df_financial = pd.read_sql(query_financial, conn)
```

4.	Checking for Missing Values:

- After loading the data into pandas, the first step in cleaning was to identify any missing values across all tables. The .isnull().sum() function was used to count the missing values in each column.
- Example:
```python 
print(df_financial.isnull().sum())
```
![is_null_sum](https://github.com/user-attachments/assets/4552c4be-51af-4a28-b496-96729c3b158e)

5.	Ensuring Data Completeness:

- The data was checked to ensure that all expected years (2013-2023) were present across all tables. This was a critical step to guarantee consistency across the various datasets.
- Example:
```python 
print(df_financial['year'].unique())
```
![year_unique](https://github.com/user-attachments/assets/e071221c-5c7c-4881-b1c0-348ba84c4376)

6.	Data Type Consistency:

- The .info() method was used to verify that the data types of each column were correct and suitable for analysis. This included ensuring that numerical columns were in the correct decimal format and categorical columns were strings.
- Example:
```python 
print(df_financial.info())
```
![table_info](https://github.com/user-attachments/assets/b0f9bda4-eef5-4f99-944b-01970ba741e1)
